{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20bf20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "\n",
    "# data parsing and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "# source paths\n",
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107b9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ddi_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    for sentence in root.findall('.//sentence'):\n",
    "        text = sentence.attrib['text']\n",
    "        text_id = sentence.attrib['id']\n",
    "        pairs = []\n",
    "        \n",
    "        for pair in sentence.findall('.//pair'):\n",
    "            e1_id = pair.attrib['e1']\n",
    "            e2_id = pair.attrib['e2']\n",
    "            ddi = pair.attrib['ddi'] == 'true'\n",
    "            ddi_type = pair.attrib.get('type', 'unknown')\n",
    "            pairs.append((e1_id, e2_id, ddi, ddi_type))\n",
    "        \n",
    "        entities = {}\n",
    "        for entity in sentence.findall('.//entity'):\n",
    "            char_offset_str = entity.attrib['charOffset']\n",
    "            entity_spans = char_offset_str.split(';')\n",
    "            spans = []\n",
    "            for span in entity_spans:\n",
    "                start, end = map(int, span.split('-'))\n",
    "                spans.append((start, end))\n",
    "            if spans:\n",
    "                entities[entity.attrib['id']] = {\n",
    "                    'text': entity.attrib['text'],\n",
    "                    'type': entity.attrib['type'],\n",
    "                    'char_offset': spans\n",
    "                }\n",
    "            else:\n",
    "                entities[entity.attrib['id']] = {\n",
    "                    'text': entity.attrib['text'],\n",
    "                    'type': entity.attrib['type'],\n",
    "                    'char_offset': None\n",
    "                }\n",
    "        \n",
    "        for e1_id, e2_id, ddi, ddi_type in pairs:\n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sentence': text,\n",
    "                'sent_id': text_id,\n",
    "                'entity1': {\n",
    "                    'text': entities[e1_id]['text'],\n",
    "                    'type': entities[e1_id]['type'],\n",
    "                    'char_offset': entities[e1_id]['char_offset']\n",
    "                },\n",
    "                'entity2': {\n",
    "                    'text': entities[e2_id]['text'],\n",
    "                    'type': entities[e2_id]['type'],\n",
    "                    'char_offset': entities[e2_id]['char_offset']\n",
    "                },\n",
    "                'ddi': ddi,\n",
    "                'type': ddi_type,\n",
    "                'all_ents': entities\n",
    "            })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preprocess_data(*corpus_paths, parse_function):\n",
    "    \"\"\"\n",
    "    Preprocess corpora of XML files for entity extraction with provided paths and parsing function.\n",
    "    Returns a list of sentences with entity/relationship pairs\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for corpus_path in corpus_paths:\n",
    "        for file in os.listdir(corpus_path):\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(corpus_path, file)\n",
    "                data.extend(parse_function(file_path))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_ner_data(data):\n",
    "    ner_data = []\n",
    "    for instance in data:\n",
    "        input_data = {}\n",
    "        input_data['sentence'] = instance['sentence']\n",
    "        input_data['entities'] = []\n",
    "        for value in instance['all_ents'].values():\n",
    "            input_data['entities'].append(value)\n",
    "        ner_data.append(input_data)\n",
    "    return ner_data\n",
    "\n",
    "def extract_ddi_data(data):\n",
    "    ddi_data = []\n",
    "    for instance in data:\n",
    "        input_data = {}\n",
    "        input_data['relations'] = {}\n",
    "        \n",
    "        input_data['sentence'] = instance['sentence']\n",
    "        input_data['relations']['entity1'] = instance['entity1']\n",
    "        input_data['relations']['entity2'] = instance['entity2']\n",
    "        input_data['relations']['ddi'] = instance['ddi']\n",
    "        input_data['relations']['type'] = instance['type']\n",
    "        \n",
    "        ddi_data.append(input_data)\n",
    "    return ddi_data\n",
    "\n",
    "def create_train_val_split(data, val_split=0.1):\n",
    "    \"\"\"\n",
    "    shuffles and splits the data \n",
    "    \"\"\"\n",
    "    random.shuffle(data)\n",
    "    val_size = int(len(data) * val_split)\n",
    "    train_data = data[:-val_size]\n",
    "    val_data = data[-val_size:]\n",
    "    return train_data, val_data\n",
    "\n",
    "def generate_conll_format(sentence, entities):\n",
    "    # Initialize an empty list to store token labels\n",
    "    labels = ['O'] * len(sentence)\n",
    "\n",
    "    # Assign labels using character offsets\n",
    "    for entity in entities:\n",
    "        span = entity['char_offset']\n",
    "        for start, end in span:\n",
    "            if start < len(sentence) and end <= len(sentence):\n",
    "                labels[start] = f\"B-{entity['type']}\"\n",
    "                for i in range(start + 1, end):\n",
    "                    labels[i] = f\"I-{entity['type']}\"\n",
    "\n",
    "    # Tokenize the sentence using regex to split on whitespace or special characters\n",
    "    pattern = r\"(\\w+|\\S)\"\n",
    "    tokens = [match.group() for match in re.finditer(pattern, sentence)]\n",
    "\n",
    "    # Combine tokens and labels into the CoNLL format\n",
    "    conll_format = {}\n",
    "    conll_format['tokens'] = []\n",
    "    conll_format['tags'] = []\n",
    "    token_start = 0\n",
    "    for token in tokens:\n",
    "        token_start = sentence.find(token, token_start)\n",
    "        token_end = token_start + len(token) - 1\n",
    "        token_label = labels[token_start:token_end+1]\n",
    "        \n",
    "        # Find the main label by checking if there are any \"B-\" or \"I-\" labels in the token_label list\n",
    "        main_label = next((label for label in token_label if label.startswith(\"B-\") or label.startswith(\"I-\")), 'O')\n",
    "        \n",
    "        #conll_format += f\"{token} {main_label}\\n\"\n",
    "        conll_format['tokens'].append(token)\n",
    "        conll_format['tags'].append(main_label)\n",
    "        token_start = token_end + 1\n",
    "\n",
    "    return conll_format\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61d969",
   "metadata": {},
   "source": [
    "# DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d038dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25013, 2779, 941)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPORA_PATH = \"raw/ddi-corpus/APIforDDICorpus/DDICorpus/\"\n",
    "\n",
    "# Preprocess training data\n",
    "train_folder_drugbank = DATA_PATH + CORPORA_PATH + \"Train/DrugBank/\"\n",
    "train_folder_medline = DATA_PATH + CORPORA_PATH + \"Train/Medline/\"\n",
    "train_data = preprocess_data(train_folder_drugbank, train_folder_medline, parse_function = parse_ddi_xml)\n",
    "\n",
    "# # Preprocess test data\n",
    "test_folder_drugbank = DATA_PATH + CORPORA_PATH + \"Test/Test for DrugNER task/DrugBank/\"\n",
    "test_folder_medline = DATA_PATH + CORPORA_PATH + \"Test/Test for DrugNER task/MedLine/\"\n",
    "\n",
    "# Preprocess test data\n",
    "test_data = preprocess_data(test_folder_drugbank, test_folder_medline, parse_function = parse_ddi_xml)\n",
    "# # Split the data into training and validation sets (80% training, 20% validation)\n",
    "train_data, val_data = create_train_val_split(train_data)\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc1380f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relations': {'entity1': {'text': 'antiretroviral nucleoside analogues',\n",
       "   'type': 'drug',\n",
       "   'char_offset': [(67, 101)]},\n",
       "  'entity2': {'text': 'ethionamide',\n",
       "   'type': 'drug',\n",
       "   'char_offset': [(153, 163)]},\n",
       "  'ddi': False,\n",
       "  'type': 'unknown'},\n",
       " 'sentence': 'Drugs that have been associated with peripheral neuropathy include antiretroviral nucleoside analogues, chloramphenicol, cisplatin, dapsone, disulfiram, ethionamide, glutethimide, gold, hydralazine, iodoquinol, isoniazid, metronidazole, nitrofurantoin, phenytoin, ribavirin, and vincristine.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddi_train_data = extract_ddi_data(train_data)\n",
    "ddi_train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06bc6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NER Entities and put them in CoNLL format\n",
    "conll_sents_train = []\n",
    "ner_data = extract_ner_data(train_data)\n",
    "for data in ner_data:\n",
    "    conll_sents_train.append(generate_conll_format(data['sentence'], data['entities']))\n",
    "    \n",
    "conll_sents_dev = []\n",
    "ner_data = extract_ner_data(val_data)\n",
    "for data in ner_data:\n",
    "    conll_sents_dev.append(generate_conll_format(data['sentence'], data['entities']))\n",
    "    \n",
    "conll_sents_test = []\n",
    "ner_data = extract_ner_data(test_data)\n",
    "for data in ner_data:\n",
    "    conll_sents_test.append(generate_conll_format(data['sentence'], data['entities']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9d9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = DATA_PATH + \"preprocessed/ddi-corpus/drug_ner/\"\n",
    "# save the data to files\n",
    "save_data(conll_sents_train, SAVE_PATH + 'train.json')\n",
    "save_data(conll_sents_test, SAVE_PATH + 'test.json')\n",
    "save_data(conll_sents_dev, SAVE_PATH + 'dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d76ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
