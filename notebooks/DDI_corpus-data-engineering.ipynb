{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "20bf20db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "# data parsing and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import json\n",
    "\n",
    "# source paths\n",
    "DATA_PATH = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "107b9865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ddi_xml(file_path):\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "\n",
    "    filename = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    for sentence in root.findall('.//sentence'):\n",
    "        text = sentence.attrib['text']\n",
    "        text_id = sentence.attrib['id']\n",
    "        pairs = []\n",
    "        \n",
    "        for pair in sentence.findall('.//pair'):\n",
    "            e1_id = pair.attrib['e1']\n",
    "            e2_id = pair.attrib['e2']\n",
    "            ddi = pair.attrib['ddi'] == 'true'\n",
    "            ddi_type = pair.attrib.get('type', 'unknown')\n",
    "            pairs.append((e1_id, e2_id, ddi, ddi_type))\n",
    "        \n",
    "        entities = {}\n",
    "        for entity in sentence.findall('.//entity'):\n",
    "            char_offset_str = entity.attrib['charOffset']\n",
    "            entity_spans = char_offset_str.split(';')\n",
    "            spans = []\n",
    "            for span in entity_spans:\n",
    "                start, end = map(int, span.split('-'))\n",
    "                spans.append((start, end))\n",
    "            if spans:\n",
    "                entities[entity.attrib['id']] = {\n",
    "                    'text': entity.attrib['text'],\n",
    "                    'type': entity.attrib['type'],\n",
    "                    'char_offset': spans\n",
    "                }\n",
    "            else:\n",
    "                entities[entity.attrib['id']] = {\n",
    "                    'text': entity.attrib['text'],\n",
    "                    'type': entity.attrib['type'],\n",
    "                    'char_offset': None\n",
    "                }\n",
    "        \n",
    "        for e1_id, e2_id, ddi, ddi_type in pairs:\n",
    "            data.append({\n",
    "                'filename': filename,\n",
    "                'sentence': text,\n",
    "                'sent_id': text_id,\n",
    "                'entity1': {\n",
    "                    'text': entities[e1_id]['text'],\n",
    "                    'type': entities[e1_id]['type'],\n",
    "                    'char_offset': entities[e1_id]['char_offset']\n",
    "                },\n",
    "                'entity2': {\n",
    "                    'text': entities[e2_id]['text'],\n",
    "                    'type': entities[e2_id]['type'],\n",
    "                    'char_offset': entities[e2_id]['char_offset']\n",
    "                },\n",
    "                'ddi': ddi,\n",
    "                'type': ddi_type,\n",
    "                'all_ents': entities\n",
    "            })\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preprocess_data(*corpus_paths, parse_function):\n",
    "    \"\"\"\n",
    "    Preprocess corpora of XML files for entity extraction with provided paths and parsing function.\n",
    "    Returns a list of sentences with entity/relationship pairs\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for corpus_path in corpus_paths:\n",
    "        for file in os.listdir(corpus_path):\n",
    "            if file.endswith('.xml'):\n",
    "                file_path = os.path.join(corpus_path, file)\n",
    "                data.extend(parse_function(file_path))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extract_ner_data(input_data):\n",
    "    ner_data = []\n",
    "    data = copy.deepcopy(input_data)\n",
    "    # Iterate through each instance in the data\n",
    "    for instance in data:\n",
    "        input_data = {}\n",
    "        input_data['sentence'] = instance['sentence']\n",
    "        input_data['entities'] = []\n",
    "\n",
    "        # Check if there is a DDI (drug-drug interaction) in the instance\n",
    "        if instance['ddi']:\n",
    "            char_offset1 = instance['entity1']['char_offset']\n",
    "            char_offset2 = instance['entity2']['char_offset']\n",
    "        else:\n",
    "            # Set dummy values if there is no DDI\n",
    "            char_offset1, char_offset2 = -1, -1\n",
    "\n",
    "        # Iterate through all entities in the instance\n",
    "        for value in instance['all_ents'].values():\n",
    "            # Check if the entity has a relation (based on char_offset)\n",
    "            if (value['char_offset'] == char_offset1) or (value['char_offset'] == char_offset2):\n",
    "                # print(value['char_offset'], char_offset1, char_offset2)\n",
    "                # Update the entity type to include the relation type\n",
    "                relation_type = value['type']\n",
    "#                 value['type'] = f\"{value['type']}-{instance['type']}\"\n",
    "                relation_entity = value.copy()\n",
    "                relation_entity['type'] = relation_type + f\"-{instance['type']}\"\n",
    "                input_data['entities'].append(relation_entity)\n",
    "            else:\n",
    "            # Append the entity to the input_data\n",
    "                input_data['entities'].append(value)\n",
    "\n",
    "        # Append the input_data to the ner_data\n",
    "        ner_data.append(input_data)\n",
    "    \n",
    "    return ner_data\n",
    "\n",
    "def extract_ddi_data(data):\n",
    "    ddi_data = []\n",
    "    for instance in data:\n",
    "        input_data = {}\n",
    "        input_data['relations'] = {}\n",
    "        \n",
    "        input_data['sentence'] = instance['sentence']\n",
    "        input_data['relations']['entity1'] = instance['entity1']\n",
    "        input_data['relations']['entity2'] = instance['entity2']\n",
    "        input_data['relations']['ddi'] = instance['ddi']\n",
    "        input_data['relations']['type'] = instance['type']\n",
    "        \n",
    "        ddi_data.append(input_data)\n",
    "    return ddi_data\n",
    "\n",
    "def create_train_val_split(data, val_split=0.1):\n",
    "    \"\"\"\n",
    "    shuffles and splits the data \n",
    "    \"\"\"\n",
    "    random.shuffle(data)\n",
    "    val_size = int(len(data) * val_split)\n",
    "    train_data = data[:-val_size]\n",
    "    val_data = data[-val_size:]\n",
    "    return train_data, val_data\n",
    "\n",
    "# def generate_conll_format(data):\n",
    "#     # Initialize an empty list to store token labels\n",
    "#     sentence = data['sentence']\n",
    "#     entities = data['entities']\n",
    "#     #relations = data['relationships']\n",
    "    \n",
    "#     labels = ['O'] * len(sentence)\n",
    "\n",
    "#     # Assign labels using character offsets\n",
    "#     for entity in entities:\n",
    "#         span = entity['char_offset']\n",
    "#         for start, end in span:\n",
    "#             if start < len(sentence) and end <= len(sentence):\n",
    "#                 labels[start] = f\"B-{entity['type']}\"\n",
    "#                 for i in range(start + 1, end):\n",
    "#                     labels[i] = f\"I-{entity['type']}\"\n",
    "\n",
    "#     # Tokenize the sentence using regex to split on whitespace or special characters\n",
    "#     pattern = r\"(\\w+|\\S)\"\n",
    "#     tokens = [match.group() for match in re.finditer(pattern, sentence)]\n",
    "\n",
    "#     # Combine tokens and labels into the CoNLL format\n",
    "#     conll_format = {}\n",
    "#     conll_format['tokens'] = []\n",
    "#     conll_format['tags'] = []\n",
    "#     token_start = 0\n",
    "#     for token in tokens:\n",
    "#         token_start = sentence.find(token, token_start)\n",
    "#         token_end = token_start + len(token) - 1\n",
    "#         token_label = labels[token_start:token_end+1]\n",
    "        \n",
    "#         # Find the main label by checking if there are any \"B-\" or \"I-\" labels in the token_label list\n",
    "#         main_label = next((label for label in token_label if label.startswith(\"B-\") or label.startswith(\"I-\")), 'O')\n",
    "        \n",
    "#         #conll_format += f\"{token} {main_label}\\n\"\n",
    "#         conll_format['tokens'].append(token)\n",
    "#         conll_format['tags'].append(main_label)\n",
    "#         token_start = token_end + 1\n",
    "#     #conll_format['relations'] = relations\n",
    "#     return conll_format\n",
    "\n",
    "def generate_conll_format(data, return_labels=False):\n",
    "    sentence = data['sentence']\n",
    "    entities = data['entities']\n",
    "    \n",
    "    labels = ['O'] * len(sentence)\n",
    "\n",
    "    # Assign labels using character offsets\n",
    "    for entity in entities:\n",
    "        span = entity['char_offset']\n",
    "        for start, end in span:\n",
    "            if start < len(sentence) and end <= len(sentence):\n",
    "                labels[start] = f\"B-{entity['type']}\"\n",
    "                for i in range(start + 1, end + 1):\n",
    "                    labels[i] = f\"I-{entity['type']}\"\n",
    "    \n",
    "    # Tokenize the sentence using regex to split on whitespace or special characters\n",
    "    pattern = r\"(\\w+|\\S)\"\n",
    "    tokens = [match.group() for match in re.finditer(pattern, sentence)]\n",
    "\n",
    "    # Combine tokens and labels into the CoNLL format\n",
    "    conll_format = {}\n",
    "    conll_format['tokens'] = []\n",
    "    conll_format['tags'] = []\n",
    "    token_start = 0\n",
    "    for token in tokens:\n",
    "        token_start = sentence.find(token, token_start)\n",
    "        token_end = token_start + len(token) - 1\n",
    "        token_label = labels[token_start:token_end + 1]\n",
    "        \n",
    "        # Find the main label by checking if there are any \"B-\" or \"I-\" labels in the token_label list\n",
    "        main_label = next((label for label in token_label if label.startswith(\"B-\") or label.startswith(\"I-\")), 'O')\n",
    "        \n",
    "        conll_format['tokens'].append(token)\n",
    "        conll_format['tags'].append(main_label)\n",
    "        token_start = token_end + 1\n",
    "    \n",
    "    if return_labels:\n",
    "        return conll_format, labels\n",
    "    else:\n",
    "        return conll_format\n",
    "\n",
    "def save_data(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61d969",
   "metadata": {},
   "source": [
    "# DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "5d038dd8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25013, 2779, 941)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPORA_PATH = \"raw/ddi-corpus/APIforDDICorpus/DDICorpus/\"\n",
    "\n",
    "# Preprocess training data\n",
    "train_folder_drugbank = DATA_PATH + CORPORA_PATH + \"Train/DrugBank/\"\n",
    "train_folder_medline = DATA_PATH + CORPORA_PATH + \"Train/Medline/\"\n",
    "train_data = preprocess_data(train_folder_drugbank, train_folder_medline, parse_function = parse_ddi_xml)\n",
    "\n",
    "# # Preprocess test data\n",
    "test_folder_drugbank = DATA_PATH + CORPORA_PATH + \"Test/Test for DrugNER task/DrugBank/\"\n",
    "test_folder_medline = DATA_PATH + CORPORA_PATH + \"Test/Test for DrugNER task/MedLine/\"\n",
    "\n",
    "# Preprocess test data\n",
    "test_data = preprocess_data(test_folder_drugbank, test_folder_medline, parse_function = parse_ddi_xml)\n",
    "# # Split the data into training and validation sets (80% training, 20% validation)\n",
    "train_data, val_data = create_train_val_split(train_data)\n",
    "len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "39a576af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3614, 388)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find some samples to check functions\n",
    "ddi_samples = []\n",
    "multi_span_samples = []\n",
    "for i, data in enumerate(train_data):\n",
    "    if data['ddi']:\n",
    "        ddi_samples.append(i)\n",
    "    for ent in data['all_ents'].values():\n",
    "        if len(ent['char_offset']) > 1:\n",
    "            multi_span_samples.append(i)\n",
    "            \n",
    "len(ddi_samples), len(multi_span_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2754149e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Allopurinol',\n",
       "  ':',\n",
       "  'The',\n",
       "  'AUC',\n",
       "  'of',\n",
       "  'didanosine',\n",
       "  'was',\n",
       "  'increased',\n",
       "  'about',\n",
       "  '4',\n",
       "  '-',\n",
       "  'fold',\n",
       "  'when',\n",
       "  'allopurinol',\n",
       "  'at',\n",
       "  '300',\n",
       "  'mg',\n",
       "  '/',\n",
       "  'day',\n",
       "  'was',\n",
       "  'coadministered',\n",
       "  'with',\n",
       "  'a',\n",
       "  'single',\n",
       "  '200',\n",
       "  '-',\n",
       "  'mg',\n",
       "  'dose',\n",
       "  'of',\n",
       "  'VIDEX',\n",
       "  'to',\n",
       "  'two',\n",
       "  'patients',\n",
       "  'with',\n",
       "  'renal',\n",
       "  'impairment',\n",
       "  '(',\n",
       "  'CLcr',\n",
       "  '=',\n",
       "  '15',\n",
       "  'and',\n",
       "  '18',\n",
       "  'mL',\n",
       "  '/',\n",
       "  'min',\n",
       "  ')',\n",
       "  '.'],\n",
       " 'tags': ['B-drug',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-drug',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-drug',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-brand',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_sents_train[multi_span_samples[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3f2f2ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'When other antiplatelet agents or anticoagulants are used concomitantly, there is the potential for FLOLAN to increase the risk of bleeding.',\n",
       " 'entities': [{'text': 'antiplatelet agents',\n",
       "   'type': 'group',\n",
       "   'char_offset': [(11, 29)]},\n",
       "  {'text': 'anticoagulants',\n",
       "   'type': 'group-effect',\n",
       "   'char_offset': [(34, 47)]},\n",
       "  {'text': 'FLOLAN', 'type': 'brand-effect', 'char_offset': [(100, 105)]}]}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ner_data([train_data[ddi_samples[1]]])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "775f7f84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': 'Dofetilide_ddi',\n",
       " 'sentence': 'If a patient requires TIKOSYN and anti-ulcer therapy, it is suggested that omeprazole, ranitidine, or antacids (aluminum and magnesium hydroxides) be used as alternatives to cimetidine, as these agents have no effect on the pharmacokinetic profile of TIKOSYN.',\n",
       " 'sent_id': 'DDI-DrugBank.d558.s5',\n",
       " 'entity1': {'text': 'aluminum hydroxide',\n",
       "  'type': 'drug',\n",
       "  'char_offset': [(112, 119), (135, 143)]},\n",
       " 'entity2': {'text': 'magnesium hydroxide',\n",
       "  'type': 'drug',\n",
       "  'char_offset': [(125, 143)]},\n",
       " 'ddi': False,\n",
       " 'type': 'unknown',\n",
       " 'all_ents': {'DDI-DrugBank.d558.s5.e0': {'text': 'TIKOSYN',\n",
       "   'type': 'brand',\n",
       "   'char_offset': [(22, 28)]},\n",
       "  'DDI-DrugBank.d558.s5.e1': {'text': 'anti-ulcer',\n",
       "   'type': 'group',\n",
       "   'char_offset': [(34, 43)]},\n",
       "  'DDI-DrugBank.d558.s5.e2': {'text': 'omeprazole',\n",
       "   'type': 'drug',\n",
       "   'char_offset': [(75, 84)]},\n",
       "  'DDI-DrugBank.d558.s5.e3': {'text': 'ranitidine',\n",
       "   'type': 'drug',\n",
       "   'char_offset': [(87, 96)]},\n",
       "  'DDI-DrugBank.d558.s5.e4': {'text': 'antacids',\n",
       "   'type': 'group',\n",
       "   'char_offset': [(102, 109)]},\n",
       "  'DDI-DrugBank.d558.s5.e5': {'text': 'aluminum hydroxide',\n",
       "   'type': 'drug',\n",
       "   'char_offset': [(112, 119), (135, 143)]},\n",
       "  'DDI-DrugBank.d558.s5.e6': {'text': 'magnesium hydroxide',\n",
       "   'type': 'drug',\n",
       "   'char_offset': [(125, 143)]},\n",
       "  'DDI-DrugBank.d558.s5.e7': {'text': 'cimetidine',\n",
       "   'type': 'drug',\n",
       "   'char_offset': [(174, 183)]},\n",
       "  'DDI-DrugBank.d558.s5.e8': {'text': 'TIKOSYN',\n",
       "   'type': 'brand',\n",
       "   'char_offset': [(251, 257)]}}}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[multi_span_samples[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "70e3b116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentence': 'If a patient requires TIKOSYN and anti-ulcer therapy, it is suggested that omeprazole, ranitidine, or antacids (aluminum and magnesium hydroxides) be used as alternatives to cimetidine, as these agents have no effect on the pharmacokinetic profile of TIKOSYN.',\n",
       "  'entities': [{'text': 'TIKOSYN', 'type': 'brand', 'char_offset': [(22, 28)]},\n",
       "   {'text': 'anti-ulcer', 'type': 'group', 'char_offset': [(34, 43)]},\n",
       "   {'text': 'omeprazole', 'type': 'drug', 'char_offset': [(75, 84)]},\n",
       "   {'text': 'ranitidine', 'type': 'drug', 'char_offset': [(87, 96)]},\n",
       "   {'text': 'antacids', 'type': 'group', 'char_offset': [(102, 109)]},\n",
       "   {'text': 'aluminum hydroxide',\n",
       "    'type': 'drug',\n",
       "    'char_offset': [(112, 119), (135, 143)]},\n",
       "   {'text': 'magnesium hydroxide',\n",
       "    'type': 'drug',\n",
       "    'char_offset': [(125, 143)]},\n",
       "   {'text': 'cimetidine', 'type': 'drug', 'char_offset': [(174, 183)]},\n",
       "   {'text': 'TIKOSYN', 'type': 'brand', 'char_offset': [(251, 257)]}]}]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ner_data([train_data[multi_span_samples[5]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "2b52a574",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conll, labels = generate_conll_format(extract_ner_data([train_data[multi_span_samples[5]]])[0], return_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "d7805d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = zip([char for char in train_data[multi_span_samples[5]]['sentence']], labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f2b82ff6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['If',\n",
       "  'a',\n",
       "  'patient',\n",
       "  'requires',\n",
       "  'TIKOSYN',\n",
       "  'and',\n",
       "  'anti',\n",
       "  '-',\n",
       "  'ulcer',\n",
       "  'therapy',\n",
       "  ',',\n",
       "  'it',\n",
       "  'is',\n",
       "  'suggested',\n",
       "  'that',\n",
       "  'omeprazole',\n",
       "  ',',\n",
       "  'ranitidine',\n",
       "  ',',\n",
       "  'or',\n",
       "  'antacids',\n",
       "  '(',\n",
       "  'aluminum',\n",
       "  'and',\n",
       "  'magnesium',\n",
       "  'hydroxides',\n",
       "  ')',\n",
       "  'be',\n",
       "  'used',\n",
       "  'as',\n",
       "  'alternatives',\n",
       "  'to',\n",
       "  'cimetidine',\n",
       "  ',',\n",
       "  'as',\n",
       "  'these',\n",
       "  'agents',\n",
       "  'have',\n",
       "  'no',\n",
       "  'effect',\n",
       "  'on',\n",
       "  'the',\n",
       "  'pharmacokinetic',\n",
       "  'profile',\n",
       "  'of',\n",
       "  'TIKOSYN',\n",
       "  '.'],\n",
       " 'tags': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-brand',\n",
       "  'O',\n",
       "  'B-group',\n",
       "  'I-group',\n",
       "  'I-group',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-drug',\n",
       "  'O',\n",
       "  'B-drug',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-group',\n",
       "  'O',\n",
       "  'B-drug',\n",
       "  'O',\n",
       "  'B-drug',\n",
       "  'I-drug',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-drug',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-brand',\n",
       "  'O']}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6e59134e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I O\n",
      "f O\n",
      "  O\n",
      "a O\n",
      "  O\n",
      "p O\n",
      "a O\n",
      "t O\n",
      "i O\n",
      "e O\n",
      "n O\n",
      "t O\n",
      "  O\n",
      "r O\n",
      "e O\n",
      "q O\n",
      "u O\n",
      "i O\n",
      "r O\n",
      "e O\n",
      "s O\n",
      "  O\n",
      "T B-brand\n",
      "I I-brand\n",
      "K I-brand\n",
      "O I-brand\n",
      "S I-brand\n",
      "Y I-brand\n",
      "N I-brand\n",
      "  O\n",
      "a O\n",
      "n O\n",
      "d O\n",
      "  O\n",
      "a B-group\n",
      "n I-group\n",
      "t I-group\n",
      "i I-group\n",
      "- I-group\n",
      "u I-group\n",
      "l I-group\n",
      "c I-group\n",
      "e I-group\n",
      "r I-group\n",
      "  O\n",
      "t O\n",
      "h O\n",
      "e O\n",
      "r O\n",
      "a O\n",
      "p O\n",
      "y O\n",
      ", O\n",
      "  O\n",
      "i O\n",
      "t O\n",
      "  O\n",
      "i O\n",
      "s O\n",
      "  O\n",
      "s O\n",
      "u O\n",
      "g O\n",
      "g O\n",
      "e O\n",
      "s O\n",
      "t O\n",
      "e O\n",
      "d O\n",
      "  O\n",
      "t O\n",
      "h O\n",
      "a O\n",
      "t O\n",
      "  O\n",
      "o B-drug\n",
      "m I-drug\n",
      "e I-drug\n",
      "p I-drug\n",
      "r I-drug\n",
      "a I-drug\n",
      "z I-drug\n",
      "o I-drug\n",
      "l I-drug\n",
      "e I-drug\n",
      ", O\n",
      "  O\n",
      "r B-drug\n",
      "a I-drug\n",
      "n I-drug\n",
      "i I-drug\n",
      "t I-drug\n",
      "i I-drug\n",
      "d I-drug\n",
      "i I-drug\n",
      "n I-drug\n",
      "e I-drug\n",
      ", O\n",
      "  O\n",
      "o O\n",
      "r O\n",
      "  O\n",
      "a B-group\n",
      "n I-group\n",
      "t I-group\n",
      "a I-group\n",
      "c I-group\n",
      "i I-group\n",
      "d I-group\n",
      "s I-group\n",
      "  O\n",
      "( O\n",
      "a B-drug\n",
      "l I-drug\n",
      "u I-drug\n",
      "m I-drug\n",
      "i I-drug\n",
      "n I-drug\n",
      "u I-drug\n",
      "m I-drug\n",
      "  O\n",
      "a O\n",
      "n O\n",
      "d O\n",
      "  O\n",
      "m B-drug\n",
      "a I-drug\n",
      "g I-drug\n",
      "n I-drug\n",
      "e I-drug\n",
      "s I-drug\n",
      "i I-drug\n",
      "u I-drug\n",
      "m I-drug\n",
      "  I-drug\n",
      "h I-drug\n",
      "y I-drug\n",
      "d I-drug\n",
      "r I-drug\n",
      "o I-drug\n",
      "x I-drug\n",
      "i I-drug\n",
      "d I-drug\n",
      "e I-drug\n",
      "s O\n",
      ") O\n",
      "  O\n",
      "b O\n",
      "e O\n",
      "  O\n",
      "u O\n",
      "s O\n",
      "e O\n",
      "d O\n",
      "  O\n",
      "a O\n",
      "s O\n",
      "  O\n",
      "a O\n",
      "l O\n",
      "t O\n",
      "e O\n",
      "r O\n",
      "n O\n",
      "a O\n",
      "t O\n",
      "i O\n",
      "v O\n",
      "e O\n",
      "s O\n",
      "  O\n",
      "t O\n",
      "o O\n",
      "  O\n",
      "c B-drug\n",
      "i I-drug\n",
      "m I-drug\n",
      "e I-drug\n",
      "t I-drug\n",
      "i I-drug\n",
      "d I-drug\n",
      "i I-drug\n",
      "n I-drug\n",
      "e I-drug\n",
      ", O\n",
      "  O\n",
      "a O\n",
      "s O\n",
      "  O\n",
      "t O\n",
      "h O\n",
      "e O\n",
      "s O\n",
      "e O\n",
      "  O\n",
      "a O\n",
      "g O\n",
      "e O\n",
      "n O\n",
      "t O\n",
      "s O\n",
      "  O\n",
      "h O\n",
      "a O\n",
      "v O\n",
      "e O\n",
      "  O\n",
      "n O\n",
      "o O\n",
      "  O\n",
      "e O\n",
      "f O\n",
      "f O\n",
      "e O\n",
      "c O\n",
      "t O\n",
      "  O\n",
      "o O\n",
      "n O\n",
      "  O\n",
      "t O\n",
      "h O\n",
      "e O\n",
      "  O\n",
      "p O\n",
      "h O\n",
      "a O\n",
      "r O\n",
      "m O\n",
      "a O\n",
      "c O\n",
      "o O\n",
      "k O\n",
      "i O\n",
      "n O\n",
      "e O\n",
      "t O\n",
      "i O\n",
      "c O\n",
      "  O\n",
      "p O\n",
      "r O\n",
      "o O\n",
      "f O\n",
      "i O\n",
      "l O\n",
      "e O\n",
      "  O\n",
      "o O\n",
      "f O\n",
      "  O\n",
      "T B-brand\n",
      "I I-brand\n",
      "K I-brand\n",
      "O I-brand\n",
      "S I-brand\n",
      "Y I-brand\n",
      "N I-brand\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for char, label in chars:\n",
    "    print(char, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "06bc6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NER Entities and put them in CoNLL format\n",
    "conll_sents_train = []\n",
    "ner_data_train = extract_ner_data(train_data)\n",
    "for data in ner_data_train:\n",
    "    conll_sents_train.append(generate_conll_format(data))\n",
    "    \n",
    "conll_sents_dev = []\n",
    "ner_data_dev = extract_ner_data(val_data)\n",
    "for data in ner_data_dev:\n",
    "    conll_sents_dev.append(generate_conll_format(data))\n",
    "    \n",
    "conll_sents_test = []\n",
    "ner_data_test = extract_ner_data(test_data)\n",
    "for data in ner_data_test:\n",
    "    conll_sents_test.append(generate_conll_format(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b0d38cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer, max_length=70):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "\n",
    "        # tokenize word and count # of subword tokens\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # add tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # add label and multiply by subword length\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    tokenized_sentence += [tokenizer.pad_token] * (max_length - len(tokenized_sentence))\n",
    "    labels += ['O'] * (max_length - len(labels))\n",
    "    #tokenized_sentence.extend(tokenizer.pad_token * (max_length - len(tokenized_sentence)))\n",
    "    #print(tokenized_sentence)\n",
    "    \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "    attention_mask = [1 if token != tokenizer.pad_token else 0 for token in tokenized_sentence]\n",
    "    #return input_ids, attention_mask, labels\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "53c17802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "09dcf267",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = conll_sents_train[multi_span_samples[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "52a052a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokens_tags = zip(sample['tokens'], sample['tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "229385c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_tokens, tokenized_tags = tokenize_and_preserve_labels(sample['tokens'], sample['tags'], tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7381948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tokens_tags = zip(tokenized_tokens, tokenized_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7d94986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If O\n",
      "a O\n",
      "patient O\n",
      "requires O\n",
      "TIKOSYN B-brand\n",
      "and O\n",
      "anti B-group\n",
      "- I-group\n",
      "ulcer I-group\n",
      "therapy O\n",
      ", O\n",
      "it O\n",
      "is O\n",
      "suggested O\n",
      "that O\n",
      "omeprazole B-drug\n",
      ", O\n",
      "ranitidine B-drug\n",
      ", O\n",
      "or O\n",
      "antacids B-group\n",
      "( O\n",
      "aluminum B-drug\n",
      "and O\n",
      "magnesium B-drug\n",
      "hydroxides I-drug\n",
      ") O\n",
      "be O\n",
      "used O\n",
      "as O\n",
      "alternatives O\n",
      "to O\n",
      "cimetidine B-drug\n",
      ", O\n",
      "as O\n",
      "these O\n",
      "agents O\n",
      "have O\n",
      "no O\n",
      "effect O\n",
      "on O\n",
      "the O\n",
      "pharmacokinetic O\n",
      "profile O\n",
      "of O\n",
      "TIKOSYN B-brand\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for token, tag in tokens_tags:\n",
    "    print(token, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "12e6c9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if O\n",
      "a O\n",
      "patient O\n",
      "requires O\n",
      "ti B-brand\n",
      "##kos B-brand\n",
      "##yn B-brand\n",
      "and O\n",
      "anti B-group\n",
      "- I-group\n",
      "ul I-group\n",
      "##cer I-group\n",
      "therapy O\n",
      ", O\n",
      "it O\n",
      "is O\n",
      "suggested O\n",
      "that O\n",
      "om B-drug\n",
      "##ep B-drug\n",
      "##raz B-drug\n",
      "##ole B-drug\n",
      ", O\n",
      "rani B-drug\n",
      "##ti B-drug\n",
      "##dine B-drug\n",
      ", O\n",
      "or O\n",
      "ant B-group\n",
      "##ac B-group\n",
      "##ids B-group\n",
      "( O\n",
      "aluminum B-drug\n",
      "and O\n",
      "magnesium B-drug\n",
      "hydro I-drug\n",
      "##xide I-drug\n",
      "##s I-drug\n",
      ") O\n",
      "be O\n",
      "used O\n",
      "as O\n",
      "alternatives O\n",
      "to O\n",
      "ci B-drug\n",
      "##met B-drug\n",
      "##idi B-drug\n",
      "##ne B-drug\n",
      ", O\n",
      "as O\n",
      "these O\n",
      "agents O\n",
      "have O\n",
      "no O\n",
      "effect O\n",
      "on O\n",
      "the O\n",
      "ph O\n",
      "##arm O\n",
      "##aco O\n",
      "##kin O\n",
      "##etic O\n",
      "profile O\n",
      "of O\n",
      "ti B-brand\n",
      "##kos B-brand\n",
      "##yn B-brand\n",
      ". O\n",
      "[PAD] O\n",
      "[PAD] O\n"
     ]
    }
   ],
   "source": [
    "for tokenized_token, tokenized_tag in tokenized_tokens_tags:\n",
    "    print(tokenized_token, tokenized_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca07588",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
